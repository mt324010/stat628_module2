{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read first 10000 rows from json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "def read_json_nrows(nrows,filename):\n",
    "    n= 0\n",
    "    with open(filename) as f:\n",
    "        while n < nrows:\n",
    "            if n == 0:\n",
    "                line = f.readline()\n",
    "                line = json.loads(line.rstrip())\n",
    "                train = pd.DataFrame(line,index = [0])\n",
    "            else:\n",
    "                temp = pd.DataFrame(json.loads(f.readline().rstrip()),index = [n])\n",
    "                train = train.append(temp)\n",
    "            n = n+1\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_10000 = read_json_nrows(10000,'review_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31292</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Total bill for this horrible service? Over $8G...</td>\n",
       "      <td>2013-05-07 04:34:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35344</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I *adore* Travis at the Hard Rock's new Kelly ...</td>\n",
       "      <td>2017-01-14 21:30:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>152538</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "      <td>2016-11-09 20:09:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71871</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Went in for a lunch. Steak sandwich was delici...</td>\n",
       "      <td>2018-01-09 20:56:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64913</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Today was my second out of three sessions I ha...</td>\n",
       "      <td>2018-01-30 23:07:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   business_id  stars                                               text  \\\n",
       "0        31292    1.0  Total bill for this horrible service? Over $8G...   \n",
       "1        35344    5.0  I *adore* Travis at the Hard Rock's new Kelly ...   \n",
       "2       152538    5.0  I have to say that this office really has it t...   \n",
       "3        71871    5.0  Went in for a lunch. Steak sandwich was delici...   \n",
       "4        64913    1.0  Today was my second out of three sessions I ha...   \n",
       "\n",
       "                  date  \n",
       "0  2013-05-07 04:34:36  \n",
       "1  2017-01-14 21:30:33  \n",
       "2  2016-11-09 20:09:03  \n",
       "3  2018-01-09 20:56:38  \n",
       "4  2018-01-30 23:07:38  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_10000.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "import numpy as np \n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I first tried to apply the *detect* function I got a 'No features in text' error. So I have to find out which review is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id                 137207\n",
       "stars                            1\n",
       "text                            :(\n",
       "date           2017-07-18 20:31:03\n",
       "Name: 6687, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_10000.loc[6687]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This review is not language, but an emoticon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_language(text):\n",
    "    # First delete all common emoticons.\n",
    "    text = re.sub('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)','',text)\n",
    "    if re.sub('[\\W]+','',text) == '':\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, just consider emoticons as English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_lang = train_10000[train_10000.text.apply(not_language)].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_10000.loc[not_lang,'lang_type'] = 'english'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import wordpunct_tokenize\n",
    "languages_ratios = {}\n",
    "for i in range(10000):\n",
    "    tokens = wordpunct_tokenize(train_10000.text[i])\n",
    "    words = [word.lower() for word in tokens]\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_ratios[language] = len(common_elements)\n",
    "    most_rated_language = max(languages_ratios, key=languages_ratios.get)\n",
    "    train_10000.loc[i,'lang_type'] = most_rated_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "for i in range(10000):\n",
    "    if i in not_lang:\n",
    "        continue\n",
    "    else:\n",
    "        train_10000.loc[i,'lang_type'] = detect(train_10000.text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':(']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(':[\\W]{0,1}',':(nmsl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en         9957\n",
       "fr           36\n",
       "es            3\n",
       "english       1\n",
       "de            1\n",
       "ja            1\n",
       "it            1\n",
       "Name: lang_type, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_10000.lang_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focus on English only at present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_10000_eng = train_10000[train_10000.lang_type == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common words and phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('the',), 43484),\n",
       " (('and',), 36448),\n",
       " (('I',), 27219),\n",
       " (('a',), 26196),\n",
       " (('to',), 25807),\n",
       " (('was',), 18766),\n",
       " (('of',), 15249),\n",
       " (('is',), 12814),\n",
       " (('for',), 12221),\n",
       " (('in',), 11026),\n",
       " (('it',), 9188),\n",
       " (('The',), 9180),\n",
       " (('with',), 8640),\n",
       " (('my',), 8424),\n",
       " (('that',), 8146),\n",
       " (('but',), 7073),\n",
       " (('on',), 7003),\n",
       " (('have',), 6583),\n",
       " (('you',), 6540),\n",
       " (('this',), 6388)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(train_10000_eng.text.values)\n",
    "text_trigrams = [i for i in ngrams(text.split(), 1)]\n",
    "Counter(text_trigrams).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 3443),\n",
       " (('and', 'the'), 2772),\n",
       " (('in', 'the'), 2722),\n",
       " (('it', 'was'), 2336),\n",
       " (('I', 'was'), 2307),\n",
       " (('on', 'the'), 2107),\n",
       " (('and', 'I'), 2017),\n",
       " (('to', 'the'), 1836),\n",
       " (('for', 'the'), 1732),\n",
       " (('for', 'a'), 1732),\n",
       " (('I', 'had'), 1559),\n",
       " (('I', 'have'), 1490),\n",
       " (('is', 'a'), 1378),\n",
       " (('to', 'be'), 1358),\n",
       " (('was', 'a'), 1313),\n",
       " (('this', 'place'), 1278),\n",
       " (('with', 'the'), 1262),\n",
       " (('at', 'the'), 1245),\n",
       " (('with', 'a'), 1216),\n",
       " (('to', 'get'), 1206)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(train_10000_eng.text.values)\n",
    "text_trigrams = [i for i in ngrams(text.split(), 2)]\n",
    "Counter(text_trigrams).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('and', 'it', 'was'), 516),\n",
       " (('one', 'of', 'the'), 451),\n",
       " (('a', 'lot', 'of'), 392),\n",
       " (('I', 'had', 'the'), 338),\n",
       " (('This', 'place', 'is'), 316),\n",
       " (('I', 'have', 'been'), 265),\n",
       " (('I', 'had', 'to'), 262),\n",
       " (('the', 'food', 'was'), 254),\n",
       " (('I', 'ordered', 'the'), 247),\n",
       " (('it', 'was', 'a'), 244),\n",
       " (('of', 'the', 'best'), 239),\n",
       " (('The', 'food', 'was'), 237),\n",
       " (('I', 'had', 'a'), 236),\n",
       " (('some', 'of', 'the'), 206),\n",
       " (('The', 'food', 'is'), 199),\n",
       " (('this', 'place', 'is'), 198),\n",
       " (('The', 'service', 'was'), 187),\n",
       " (('the', 'food', 'is'), 187),\n",
       " (('to', 'get', 'a'), 182),\n",
       " (('This', 'is', 'a'), 181)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(train_10000_eng.text.values)\n",
    "text_trigrams = [i for i in ngrams(text.split(), 3)]\n",
    "Counter(text_trigrams).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('one', 'of', 'the', 'best'), 126),\n",
       " (('My', 'husband', 'and', 'I'), 88),\n",
       " (('I', 'will', 'definitely', 'be'), 77),\n",
       " (('is', 'one', 'of', 'the'), 73),\n",
       " (('the', 'end', 'of', 'the'), 67),\n",
       " (('in', 'the', 'middle', 'of'), 65),\n",
       " (('for', 'the', 'first', 'time'), 65),\n",
       " (('a', 'great', 'place', 'to'), 62),\n",
       " (('some', 'of', 'the', 'best'), 60),\n",
       " (('and', 'the', 'service', 'was'), 56),\n",
       " (('you', 'are', 'looking', 'for'), 55),\n",
       " (('was', 'one', 'of', 'the'), 54),\n",
       " (('and', 'the', 'food', 'was'), 53),\n",
       " (('I', 'have', 'to', 'say'), 52),\n",
       " (('My', 'wife', 'and', 'I'), 50),\n",
       " (('and', 'the', 'food', 'is'), 50),\n",
       " (('was', 'my', 'first', 'time'), 49),\n",
       " (('one', 'of', 'my', 'favorite'), 49),\n",
       " (('the', 'rest', 'of', 'the'), 48),\n",
       " (('I', \"can't\", 'wait', 'to'), 47)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(train_10000_eng.text.values)\n",
    "text_trigrams = [i for i in ngrams(text.split(), 4)]\n",
    "Counter(text_trigrams).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oddest words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('qualified,',), 1),\n",
       " (('exuded',), 1),\n",
       " ((\"'cool'\",), 1),\n",
       " (('Danny.',), 1),\n",
       " (('accord',), 1),\n",
       " (('b-day.',), 1),\n",
       " (('Terroni.',), 1),\n",
       " (('raptor',), 1),\n",
       " (('flats',), 1),\n",
       " (('gripe...no',), 1),\n",
       " (('GPS?',), 1),\n",
       " (('yardage',), 1),\n",
       " (('caliber,',), 1),\n",
       " (('troon',), 1),\n",
       " (('Quintero',), 1),\n",
       " (('parking....my',), 1),\n",
       " (('pefect',), 1),\n",
       " (('Dewy',), 1),\n",
       " (('appoinment,',), 1),\n",
       " (('in...)',), 1),\n",
       " (('HVAC/Electrical/etc...',), 1),\n",
       " (('electrical....(I',), 1),\n",
       " (('NHW',), 1),\n",
       " (('compressor',), 1),\n",
       " (('verified',), 1),\n",
       " (('TUESDAY',), 1),\n",
       " (('compressor....',), 1),\n",
       " (('occasions....',), 1),\n",
       " (('SH!*',), 1),\n",
       " (('Moe',), 1),\n",
       " (('matted',), 1),\n",
       " (('Choosing',), 1),\n",
       " (('changing,',), 1),\n",
       " (('eats!',), 1),\n",
       " (('loong',), 1),\n",
       " (('lo-mean,',), 1),\n",
       " (('Discovered',), 1),\n",
       " (('Beach.',), 1),\n",
       " (('Thrilled',), 1),\n",
       " (('favs:',), 1),\n",
       " (('DELISH!)',), 1),\n",
       " (('is..',), 1),\n",
       " (('Woo',), 1),\n",
       " (('Che.',), 1),\n",
       " (('grilles,',), 1),\n",
       " (('bugogi',), 1),\n",
       " (('thicker,',), 1),\n",
       " (('Louie,',), 1),\n",
       " (('hmmm,',), 1),\n",
       " (('names),',), 1),\n",
       " ((\"'flatbread'\",), 1),\n",
       " ((\"'tacos',\",), 1),\n",
       " (('specialized?',), 1),\n",
       " ((\"'asian\",), 1),\n",
       " ((\"sauce',\",), 1),\n",
       " ((\"'12'\",), 1),\n",
       " ((\"'5',\",), 1),\n",
       " (('singing,',), 1),\n",
       " ((\"'it\",), 1),\n",
       " ((\"somewhere',\",), 1),\n",
       " ((\"'let's\",), 1),\n",
       " ((\"drink'!\",), 1),\n",
       " (('hoopla',), 1),\n",
       " (('pedestrians.',), 1),\n",
       " (('t.o.',), 1),\n",
       " (('piazza...might',), 1),\n",
       " (('cobblestones',), 1),\n",
       " (('though...better',), 1),\n",
       " (('distillery.',), 1),\n",
       " (('food/drinks',), 1),\n",
       " (('scoreboard',), 1),\n",
       " (('keeps!',), 1),\n",
       " (('amusing',), 1),\n",
       " (('experience.Ordered',), 1),\n",
       " (('Yada.',), 1),\n",
       " (('Dina',), 1),\n",
       " (('Yemen',), 1),\n",
       " (('master.',), 1),\n",
       " (('Samples',), 1),\n",
       " (('EZMart',), 1),\n",
       " (('Ames.',), 1),\n",
       " (('saxby',), 1),\n",
       " (('denizens',), 1),\n",
       " (('bodied',), 1),\n",
       " (('A+++',), 1),\n",
       " (('Ultra',), 1),\n",
       " (('Cooling.',), 1),\n",
       " (('&Pylte',), 1),\n",
       " (('YUMMMMMMMMMMMMMMMMM',), 1),\n",
       " (('consumer.',), 1),\n",
       " (('finish...',), 1),\n",
       " (('Africa...\"',), 1),\n",
       " (('\"famous\".',), 1),\n",
       " (('zagat',), 1),\n",
       " (('exaggerated',), 1),\n",
       " (('Dor-Stop.',), 1),\n",
       " (('incredible?',), 1),\n",
       " (('jumbot,',), 1),\n",
       " (('verbally.',), 1),\n",
       " (('catch.',), 1),\n",
       " (('flavor/seasoning',), 1),\n",
       " (('(tasted',), 1),\n",
       " (('powder-based).',), 1),\n",
       " (('wetter',), 1),\n",
       " (('scrambled.',), 1),\n",
       " (('Edmonton,',), 1),\n",
       " (('MEH.',), 1),\n",
       " (('advantage)',), 1),\n",
       " (('\"Hi\"',), 1),\n",
       " (('you?\")',), 1),\n",
       " (('thrusting',), 1),\n",
       " (('fliers',), 1),\n",
       " (('WITNESSED',), 1),\n",
       " (('guarentee',), 1),\n",
       " (('drummed',), 1),\n",
       " (('expire).',), 1),\n",
       " (('cringing',), 1),\n",
       " (('sip).',), 1),\n",
       " (('GOING',), 1),\n",
       " (('distractions)',), 1),\n",
       " (('13/30',), 1),\n",
       " (('Froyo',), 1),\n",
       " (('Kokomo.',), 1),\n",
       " (('networking',), 1),\n",
       " (('colleagues!',), 1),\n",
       " (('sport.',), 1),\n",
       " (('Hotels.com',), 1),\n",
       " (('CPKs,',), 1),\n",
       " (('top-of-the-line',), 1),\n",
       " (('strip-center',), 1),\n",
       " (('(save',), 1),\n",
       " (('cleanest',), 1),\n",
       " (('cooked.Not',), 1),\n",
       " (('Before,',), 1),\n",
       " (('\"French',), 1),\n",
       " (('Grocery\"',), 1),\n",
       " (('\"French\",',), 1),\n",
       " (('\"Grocery\"',), 1),\n",
       " (('tostadas.',), 1),\n",
       " (('chef!!',), 1),\n",
       " (('antipasta.',), 1),\n",
       " (('waiter\"Jordan\"',), 1),\n",
       " (('Hawaii?',), 1),\n",
       " (('mega-bucks,',), 1),\n",
       " (('Franklin',), 1),\n",
       " (('Crafts,',), 1),\n",
       " (('Wal-Mart',), 1),\n",
       " (('crafter',), 1),\n",
       " (('Oahu',), 1),\n",
       " (('California:',), 1),\n",
       " (('beads,',), 1),\n",
       " (('baubles,',), 1),\n",
       " (('frames,',), 1),\n",
       " (('Franklin,',), 1),\n",
       " (('cashieer',), 1),\n",
       " (('super-friendly',), 1),\n",
       " (('cafe~',), 1),\n",
       " (('Coffees,',), 1),\n",
       " (('well~',), 1),\n",
       " (('\"minor\"',), 1),\n",
       " (('$54',), 1),\n",
       " (('skyrocket',), 1),\n",
       " (('15-18',), 1),\n",
       " (('clinica',), 1),\n",
       " (('familia',), 1),\n",
       " (('expensive?',), 1),\n",
       " (('guys?',), 1),\n",
       " (('slices...on',), 1),\n",
       " (('Henderson,',), 1),\n",
       " (('Sam.',), 1),\n",
       " (('lesbians.',), 1),\n",
       " (('LGBT',), 1),\n",
       " (('...weird)',), 1),\n",
       " (('massages,',), 1),\n",
       " (('(peeled',), 1),\n",
       " (('eat)',), 1),\n",
       " (('Ethnic',), 1),\n",
       " (('Cajun,',), 1),\n",
       " (('2:30pm.',), 1),\n",
       " (('Arizona...',), 1),\n",
       " (('appreciative.',), 1),\n",
       " (('elote',), 1),\n",
       " (('specialists.',), 1),\n",
       " (('5:30pm',), 1),\n",
       " (('\"ok\",',), 1),\n",
       " (('under.',), 1),\n",
       " (('Hhhhhhoooorible!',), 1),\n",
       " (('RECEPTIONIST',), 1),\n",
       " (('HUFFED',), 1),\n",
       " (('PUFFED',), 1),\n",
       " (('\"ok,status',), 1),\n",
       " (('quo',), 1),\n",
       " (('restaurant\"',), 1),\n",
       " (('transplants',), 1),\n",
       " (('2).',), 1),\n",
       " (('seated.\"',), 1),\n",
       " (('SIT',), 1),\n",
       " (('COMMUNAL',), 1),\n",
       " (('TABLE?\"',), 1),\n",
       " (('serval',), 1),\n",
       " (('tables(besides',), 1),\n",
       " (('EMPTY!i',), 1),\n",
       " (('when?',), 1),\n",
       " (('defeat',), 1),\n",
       " (('COMMUNAL?',), 1),\n",
       " (('charge\"',), 1),\n",
       " (('side......',), 1),\n",
       " (('\"....In',), 1),\n",
       " (('54',), 1),\n",
       " (('mark!',), 1),\n",
       " (('will......and',), 1),\n",
       " (('GRADE:',), 1),\n",
       " (('FAIL!',), 1),\n",
       " (('greatest?',), 1),\n",
       " (('evolves',), 1),\n",
       " (('leaps',), 1),\n",
       " (('Roll?',), 1),\n",
       " (('scientists',), 1),\n",
       " (('instead?',), 1),\n",
       " (('sectional.',), 1),\n",
       " (('snatching',), 1),\n",
       " (('\"Sugar',), 1),\n",
       " (('Raw\",',), 1),\n",
       " (('critter',), 1),\n",
       " (('nice.)',), 1),\n",
       " (('tolerating',), 1),\n",
       " (('moi.',), 1),\n",
       " (('(outside',), 1),\n",
       " (('Savannah).',), 1),\n",
       " (('fancy-pants',), 1),\n",
       " (('chicken-y?',), 1),\n",
       " (('uber-romantic',), 1),\n",
       " (('not-great',), 1),\n",
       " (('table.....Sigh',), 1),\n",
       " (('389,',), 1),\n",
       " (('june',), 1),\n",
       " (('11,2016,',), 1),\n",
       " (('8:45pm',), 1),\n",
       " (('Airport,',), 1),\n",
       " (('Crown',), 1),\n",
       " (('junk.',), 1),\n",
       " (('TRIED',), 1),\n",
       " (('backseat.',), 1),\n",
       " (('SQUEEZED',), 1),\n",
       " (('weave',), 1),\n",
       " (('junkpile.',), 1),\n",
       " (('headrest',), 1),\n",
       " (('BLARING',), 1),\n",
       " (('Dangerously',), 1),\n",
       " (('Louis-Simon',), 1),\n",
       " (('Ca',), 1),\n",
       " (('Roule.',), 1),\n",
       " (('perched',), 1),\n",
       " (('\"Dining',), 1),\n",
       " (('Arroyo\"',), 1),\n",
       " (('dining/lounge',), 1),\n",
       " (('table),',), 1),\n",
       " (('unmotivated',), 1),\n",
       " (('upscale-casual',), 1),\n",
       " (('match..',), 1),\n",
       " (('saying..',), 1),\n",
       " (('Wiseguy.',), 1),\n",
       " (('wiseguy.',), 1),\n",
       " (('pizza),',), 1),\n",
       " (('splurging',), 1),\n",
       " (('while?',), 1),\n",
       " (('juggernauts,',), 1),\n",
       " (('LGO,',), 1),\n",
       " (('groovy.',), 1),\n",
       " (('adjective',), 1),\n",
       " (('skeletons',), 1),\n",
       " (('overshadow',), 1),\n",
       " (('interesting--short,',), 1),\n",
       " (('pie!',), 1),\n",
       " (('down),',), 1),\n",
       " (('jenga).',), 1),\n",
       " (('meal!!',), 1),\n",
       " (('\"conservative\"',), 1),\n",
       " (('Republican',), 1),\n",
       " (('mayor',), 1),\n",
       " (('\"Trib\"',), 1),\n",
       " (('Greensburg',), 1),\n",
       " (('strike.',), 1),\n",
       " (('Press,',), 1),\n",
       " (('1992',), 1),\n",
       " (('Mellon',), 1),\n",
       " (('Review,',), 1),\n",
       " (('conservative,',), 1),\n",
       " (('afloat.',), 1),\n",
       " (('1992,',), 1),\n",
       " (('\"Pittsburgh',), 1),\n",
       " (('Review\"',), 1),\n",
       " (('Democrats,',), 1),\n",
       " (('strike,',), 1),\n",
       " (('absorbed',), 1),\n",
       " (('publication',), 1),\n",
       " (('supermarkets,',), 1),\n",
       " (('stored.',), 1),\n",
       " (('comics,',), 1),\n",
       " (('obituaries',), 1),\n",
       " (('ads.',), 1),\n",
       " (('low-lit,',), 1),\n",
       " (('BYOB!',), 1),\n",
       " (('eaten!!',), 1),\n",
       " (('go-',), 1),\n",
       " (('tempeh',), 1),\n",
       " (('night!!!',), 1),\n",
       " (('lot!!!',), 1),\n",
       " (('65+',), 1),\n",
       " (('45+',), 1),\n",
       " (('helluva',), 1),\n",
       " (('meal.\"',), 1),\n",
       " (('ROACHES',), 1),\n",
       " (('不要在这里吃！',), 1),\n",
       " (('我们刚在这里吃午饭，在我们的汤里发现了一个蟑螂，我把它展示给服务员，她说：哦，对不起。你不必付饭费\"，这意味着他们的厨房里有很多蟑螂，他们知道，她一点都不惊讶。',),\n",
       "  1),\n",
       " (('ommmmmg',), 1),\n",
       " ((\"Roberto's,\",), 1),\n",
       " (('curiosity,',), 1),\n",
       " ((\"Ami's\",), 1),\n",
       " ((\"Angeles'\",), 1),\n",
       " (('Koreatown',), 1),\n",
       " (('Koreatown,',), 1),\n",
       " (('enclave',), 1),\n",
       " (('\"halmoni\"',), 1),\n",
       " (('grandkids,',), 1),\n",
       " (('\"Express\"',), 1),\n",
       " (('crib.',), 1),\n",
       " (('\"express\"',), 1),\n",
       " (('configure',), 1),\n",
       " (('respectively).',), 1),\n",
       " (('verdict?',), 1),\n",
       " (('bu,',), 1),\n",
       " (('bap,',), 1),\n",
       " (('Delicous!!',), 1),\n",
       " (('against?',), 1),\n",
       " (('altercation,',), 1),\n",
       " (('deprecation',), 1),\n",
       " (('atmospheric',), 1),\n",
       " ((\"mark...I've\",), 1),\n",
       " (('downpoint.',), 1),\n",
       " (('anyway),',), 1),\n",
       " (('fresh.....',), 1),\n",
       " ((\"Italian...it's\",), 1),\n",
       " (('waterfall/pool',), 1),\n",
       " (('$5-10',), 1),\n",
       " (('fleek!',), 1),\n",
       " (('(strange',), 1),\n",
       " (('know),',), 1),\n",
       " (('Monorail',), 1),\n",
       " (('air!',), 1),\n",
       " (('Literally,',), 1),\n",
       " (('bells',), 1),\n",
       " (('friendly;',), 1),\n",
       " (('Damian',), 1),\n",
       " (('eighty',), 1),\n",
       " (('SIDE',), 1),\n",
       " (('EFFECTS',), 1),\n",
       " (('prescribed.',), 1),\n",
       " (('is.\"',), 1),\n",
       " (('elderly!',), 1),\n",
       " ((\"Lee's.\",), 1),\n",
       " (('prob.',), 1),\n",
       " ((\"won'\",), 1),\n",
       " (('registered',), 1),\n",
       " (('psssh',), 1),\n",
       " (('bye!',), 1),\n",
       " (('blah!!!',), 1),\n",
       " (('POW',), 1),\n",
       " (('BANG.',), 1),\n",
       " ((\"Ashley's\",), 1),\n",
       " (('exhibition',), 1),\n",
       " (('animation',), 1),\n",
       " (('cartoons',), 1),\n",
       " (('SpongeBob',), 1),\n",
       " (('SquarePants,',), 1),\n",
       " (('Looney',), 1),\n",
       " (('Tunes,',), 1),\n",
       " (('beer/pint',), 1),\n",
       " (('empanada..',), 1),\n",
       " (('(running',), 1),\n",
       " (('(non-toxic',), 1),\n",
       " (('materials,',), 1),\n",
       " (('insulation,',), 1),\n",
       " (('museum!',), 1),\n",
       " (('piggy,',), 1),\n",
       " (('11:50',), 1),\n",
       " (('matinee!',), 1),\n",
       " (('Tequila.',), 1),\n",
       " (('\"one',), 1),\n",
       " (('hundred,',), 1),\n",
       " (('spiny',), 1),\n",
       " (('plants\"',), 1),\n",
       " (('that...they',), 1),\n",
       " (('tequila...lots',), 1),\n",
       " (('tequila...a',), 1),\n",
       " (('siphon',), 1),\n",
       " (('1978',), 1),\n",
       " (('Nogales',), 1),\n",
       " (('evidently,',), 1),\n",
       " (('agaves',), 1),\n",
       " (('believed).',), 1),\n",
       " (('club\"...nice!',), 1),\n",
       " (('club\"',), 1),\n",
       " (('bulletproof...I',), 1),\n",
       " (('\"tequila\"',), 1),\n",
       " (('agaves,',), 1),\n",
       " (('(whoever',), 1),\n",
       " (('\"they\"',), 1),\n",
       " (('control...I',), 1),\n",
       " (('juice:)',), 1),\n",
       " (('club...you',), 1),\n",
       " (('thanking',), 1),\n",
       " (('stars:)',), 1),\n",
       " (('euphemism...dear',), 1),\n",
       " (('privilege:)',), 1),\n",
       " (('today...no,',), 1),\n",
       " (('Tecate',), 1),\n",
       " ((\"cerveza's\",), 1),\n",
       " (('silly...dad',), 1),\n",
       " (('buying:)',), 1),\n",
       " (('BTW).',), 1),\n",
       " (('tacos...(ok,',), 1),\n",
       " (('salty...and',), 1),\n",
       " (('salty...',), 1),\n",
       " (('jam-packed',), 1),\n",
       " (('tacos...I',), 1),\n",
       " (('club:)',), 1),\n",
       " (('Haylee',), 1),\n",
       " (('y.o.',), 1),\n",
       " (('jeep.',), 1),\n",
       " (('(Camry',), 1),\n",
       " (('SE),',), 1),\n",
       " (('visually.',), 1),\n",
       " (('bleached',), 1),\n",
       " (('blond.',), 1),\n",
       " (('(Meaning',), 1),\n",
       " (('fades,',), 1),\n",
       " (('Sunny',), 1),\n",
       " (('Clay',), 1),\n",
       " (('achievement.',), 1),\n",
       " (('deliver....This',), 1),\n",
       " (('family/table',), 1),\n",
       " (('conveinent',), 1),\n",
       " (('\"Mike\"',), 1),\n",
       " (('shake.....which',), 1),\n",
       " (('Relaxing',), 1),\n",
       " (('Table-side',), 1),\n",
       " (('Humor',), 1),\n",
       " (('Physician,',), 1),\n",
       " (('Jongeys',), 1),\n",
       " (('anesthetic',), 1),\n",
       " (('Hummmm...',), 1),\n",
       " (('\"new',), 1),\n",
       " (('drugs\"',), 1),\n",
       " (('\"better\",',), 1),\n",
       " (('injections',), 1),\n",
       " (('momentary)',), 1),\n",
       " (('\"at-home\"',), 1),\n",
       " (('post-surgical',), 1),\n",
       " (('Kyou',), 1),\n",
       " (('free-gluten',), 1),\n",
       " (('(bun',), 1),\n",
       " (('requested)',), 1),\n",
       " (('whack-a-mole',), 1),\n",
       " (('\"Southern',), 1),\n",
       " (('Burger\"',), 1),\n",
       " (('spooky)',), 1),\n",
       " (('adequately',), 1),\n",
       " (('toppinged',), 1),\n",
       " (('burning,',), 1),\n",
       " (('(yea',), 1),\n",
       " (('gloves!)',), 1),\n",
       " (('reclaimed-paper',), 1),\n",
       " (('capacity.',), 1),\n",
       " (('location(',), 1),\n",
       " (('problens',), 1),\n",
       " (('york',), 1),\n",
       " (('SLS.',), 1),\n",
       " (('Landmark',), 1),\n",
       " (('Village.',), 1),\n",
       " (('cosy',), 1),\n",
       " (('$0.85',), 1),\n",
       " (('bag)',), 1),\n",
       " (('\"etsu\"just',), 1),\n",
       " (('$8.95',), 1),\n",
       " (('Toronto),',), 1),\n",
       " (('lacking/confusing',), 1),\n",
       " (('Baldwin.',), 1),\n",
       " (('Baldwin,',), 1),\n",
       " (('Konnichiwa',), 1),\n",
       " (('Bocca.',), 1),\n",
       " ((',felt',), 1),\n",
       " (('balyaged',), 1),\n",
       " (('constantly!',), 1),\n",
       " (('bellpeppers,',), 1),\n",
       " (('Pam',), 1),\n",
       " (('willingly',), 1),\n",
       " (('accommodated!',), 1),\n",
       " (('(Matt!)',), 1),\n",
       " (('visit--worth',), 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(train_10000_eng.text.values)\n",
    "text_trigrams = [i for i in ngrams(text.split(), 1)]\n",
    "Counter(text_trigrams).most_common()[-500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I found two sentences in Chinese:\n",
    "- (('不要在这里吃！',), 1),\n",
    "- (('我们刚在这里吃午饭，在我们的汤里发现了一个蟑螂，我把它展示给服务员，她说：哦，对不起。你不必付饭费\"，这意味着他们的厨房里有很多蟑螂，他们知道，她一点都不惊讶。',),  \n",
    "\n",
    "Most of these rare words have some punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('visit!', \"Haven't\"), 1),\n",
       " (('years', 'bc'), 1),\n",
       " (('something', 'drew'), 1),\n",
       " (('whites', 'mixed'), 1),\n",
       " (('with', 'bellpeppers,'), 1),\n",
       " (('bellpeppers,', 'onions'), 1),\n",
       " (('tomatoes,', 'avocado'), 1),\n",
       " (('fruit,', 'with'), 1),\n",
       " (('muffin.', 'Super'), 1),\n",
       " (('Super', 'healthy'), 1),\n",
       " (('with', 'Pam'), 1),\n",
       " (('Pam', 'instead'), 1),\n",
       " (('they', 'willingly'), 1),\n",
       " (('willingly', 'accommodated!'), 1),\n",
       " (('accommodated!', 'This'), 1),\n",
       " (('server', '(Matt!)'), 1),\n",
       " (('(Matt!)', 'was'), 1),\n",
       " (('AWESOME!', 'Come'), 1),\n",
       " (('and', 'visit--worth'), 1),\n",
       " (('visit--worth', 'it!'), 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(train_10000_eng.text.values)\n",
    "text_trigrams = [i for i in ngrams(text.split(), 2)]\n",
    "Counter(text_trigrams).most_common()[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check bad words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_emoticons(text):\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
    "    if emoticons == []:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg1 = np.where(train_10000_eng.text.apply(find_emoticons) == True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Man, I love Toronto! Hiding in a strip mall on Overlea, find a dingy looking restaurant that serves up fantastic, cheap kabob...be warned regardless of when you come here you'll likely be waiting for a table as this place is always overrun with people...If you're a fan of perfectly grilled spicy meat, you must try this place out...now that I've been here I need to limit how many times I go here per month :)\\n\\nTake the drive out and have some kabob...if you don't own a car, borrow one or get a zipcar membership...totally worth it!\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_10000_eng.iloc[eg1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can find one  :)  together with two '\\n' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Man, I love Toronto! Hiding in a strip mall on Overlea, find a dingy looking restaurant that serves up fantastic, cheap kabob...be warned regardless of when you come here you'll likely be waiting for a table as this place is always overrun with people...If you're a fan of perfectly grilled spicy meat, you must try this place out...now that I've been here I need to limit how many times I go here per month :)Take the drive out and have some kabob...if you don't own a car, borrow one or get a zipcar membership...totally worth it!\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('\\\\n','',train_10000_eng.iloc[eg1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most packages can't handle some words like 'nooooo'. \n",
    "def check_same(word):\n",
    "    intervals = {}\n",
    "    for index,letter in enumerate(word):\n",
    "        if letter == word[index-1]:\n",
    "            if letter in intervals.keys():\n",
    "                interval = intervals.pop(letter)\n",
    "                lastguy = interval[len(interval)-1] \n",
    "                if lastguy[1] == (index-1):\n",
    "                    lastguy = (lastguy[0],index)\n",
    "                    interval[len(interval)-1] = lastguy\n",
    "                    intervals[letter] = interval\n",
    "                else:\n",
    "                    lastguy1 = (index-1,index)\n",
    "                    interval.append(lastguy1)\n",
    "                    intervals[letter] = interval\n",
    "            else:\n",
    "                intervals[letter] = [(index-1,index)]\n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have never seen a word with three continuous same letter, so I will delete till two.\n",
    "def no_more_than_2(word,dupli):\n",
    "    for key in dupli.keys():\n",
    "        for interval in dupli[key]:\n",
    "            length = interval[1]-interval[0]+1\n",
    "            regex = '(%s'%key + '{%i})'%length\n",
    "            word = re.sub(regex,key+key,word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import suggest\n",
    "from itertools import combinations\n",
    "def right_spelling(word,dupli):\n",
    "    key_list = []\n",
    "    for key in dupli.keys():\n",
    "        key_list.append(key)\n",
    "    n = len(key_list)\n",
    "    for i in range(n):\n",
    "        for comb in combinations(key_list,i):\n",
    "            for letter in comb:\n",
    "                regex = '(%s'%letter + '{2})'\n",
    "                new_word = re.sub(regex,letter,word)\n",
    "                if new_word in brown.words():\n",
    "                    return new_word\n",
    "    return suggest(word)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_typo(word):\n",
    "    if len(word) == len(set(word)):\n",
    "        return word\n",
    "    if suggest(word)[0][1] == 1:\n",
    "        return suggest(word)[0][0]\n",
    "    else:\n",
    "        duplicates = check_same(word)\n",
    "        two = no_more_than_2(word,duplicates)\n",
    "        suggest_two = suggest(two)\n",
    "        if suggest_two[0][1] == 1:\n",
    "            return suggest_two[0][0]\n",
    "        else:\n",
    "            return right_spelling(two,duplicates)\n",
    "        \n",
    "        return right_spelling(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finally'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_typo('finaaallly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "wnl = WordNetLemmatizer()\n",
    "def lemmatizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmas = []\n",
    "    tagged = pos_tag(tokens)\n",
    "    for tag in tagged:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "        lemmas.append(wnl.lemmatize(tag[0], pos=wordnet_pos))\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think words which mean negative are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop.pop(stop.index('but'))\n",
    "stop.pop(stop.index('not'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert n't to not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_abbreviation(text):\n",
    "    text = re.sub('n\\'t',' not',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adversatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "but = ['yet','however','nonetheless','whereas','nevertheless']\n",
    "although = ['although','though','notwithstanding','albeit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_but(text):\n",
    "    for x in but:\n",
    "        text = re.sub(x,'but',text)\n",
    "    return text\n",
    "def change_although(text):\n",
    "    for x in although:\n",
    "        text = re.sub(x,'although',text)\n",
    "    return text\n",
    "def change_adversatives(text):\n",
    "    text = change_but(text)\n",
    "    text = change_although(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to capture the key information near but and although."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def although_phrase(text):\n",
    "    words = text.split()\n",
    "    for (index,word) in enumerate(words):\n",
    "        if word == 'altough.':\n",
    "            for x in range(index,index-10,-1):\n",
    "                if re.sub('(.*)\\.([a-z])\\..*','\\\\2',str(wordnet.synsets(words[x])[0])) in ['v','adj']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
    "\n",
    "    text = re.sub('\\\\n',' ',text)\n",
    "\n",
    "    text = no_abbreviation(text)\n",
    "\n",
    "    text = re.sub('[\\W]+',' ', text.lower())\n",
    "\n",
    "    text = change_adversatives(text)\n",
    "\n",
    "    tokens = lemmatizer(text)\n",
    "    text = ''\n",
    "    for index, token in enumerate(tokens):\n",
    "\n",
    "        #tokens[index] = no_typo(token)\n",
    "        if token in stop:\n",
    "            tokens[index] = ''\n",
    "        else:\n",
    "            text = text + tokens[index] + ' '\n",
    "    return {'text':text,'emoticons':emoticons}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9957/9957 [00:48<00:00, 204.41it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm, tqdm_pandas\n",
    "tqdm.pandas()\n",
    "dictionary = train_10000_eng.text.progress_apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons = [dictionary[i]['emoticons'] for i in train_10000_eng.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [dictionary[i]['text'] for i in train_10000_eng.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
